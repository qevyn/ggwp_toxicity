{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dota Dataset Notebook 5 - Word Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TW: This notebook contains highly offensive language.\n",
    "In order to prevent this language, we need to analyze the contexts they are used in and the players that use this type of language. Although all of the notebooks that include this dataset have offensive language appear, the work in this notebook seeks to analyze the usage of these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding word embeddings to the toxicity classifier\n",
    "   * Purpose 1: make the model more generalizable\n",
    "       * ie. being trained on \"n_gger\" and generalizing to other races, such as \"ch_nk\"\n",
    "       * typos: trash vs trsah\n",
    "   * Purpose 2: make the model more robust to attempts to get around it\n",
    "       * ie. attempting to not be caught by typing \"assh01e\" instead of \"asshole\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook covers:**\n",
    "* Finding the proper word embeddings\n",
    "* Applying word embeddings to the model, first with a normal average\n",
    "* Applying word embeddings to the model with a weighted average, the weights being from Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import langdetect as ld\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labeled_dota.csv')\n",
    "\n",
    "# When saving the table, it turned empty strings into nulls.\n",
    "df['text'] = df['text'].fillna('')\n",
    "df = df.drop('severe_toxic', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Google News Corpus\n",
    "* The Google News corpus is the most popular corpus for word embeddings. Due to this, it will be the initial starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('niggas', 0.7684844136238098),\n",
       " ('n_*_gga', 0.7659837603569031),\n",
       " ('ni_**', 0.7131214141845703),\n",
       " ('n_*_ggas', 0.7091651558876038),\n",
       " ('homie', 0.6947641372680664),\n",
       " ('Niggas', 0.6646634340286255),\n",
       " ('kanye', 0.6643739342689514),\n",
       " ('motherf_*_cker', 0.6538609266281128),\n",
       " ('**_ga', 0.6494430303573608),\n",
       " ('sh_*_t', 0.6478126049041748)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('nigga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chinks', 0.7933344841003418),\n",
       " ('Achilles_heel', 0.49940359592437744),\n",
       " ('flaw', 0.49598392844200134),\n",
       " ('soft_underbelly', 0.4720204770565033),\n",
       " ('silver_lining', 0.46761447191238403),\n",
       " ('weaknesses', 0.464680552482605),\n",
       " ('glimmer', 0.4613160490989685),\n",
       " ('silver_linings', 0.43982821702957153),\n",
       " ('flaws', 0.42893803119659424),\n",
       " ('Chinks', 0.4236713945865631)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('chink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04216005"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine similarity (preferred over euclidean distance)\n",
    "model.similarity('chink', 'asian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40189582"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('gay', 'fagot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'faggot' was not included in the vocabulary, but 'fagot' was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5408276"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('nigga', 'nigger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.071316406"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('chink', 'nigger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40419376"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('NIGGER', 'nigger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Testing it out for this purpose, it did not perform well. This conclusion came about after trying other models below.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the English Dota Dataset\n",
    "* Applying Word2Vec on the Dota dataset itself\n",
    "    * This defeats the purpose of trying to generalize our model to unseen words, but this is to observe the relationships between words within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>slot</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1808.40822</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-131.14018</td>\n",
       "      <td>twitch.tv/rage_channel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-121.60481</td>\n",
       "      <td>https://www.twitch.tv/rage_channel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>700.72893</td>\n",
       "      <td>https://www.twitch.tv/rage_channel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>702.99503</td>\n",
       "      <td>https://www.twitch.tv/rage_channel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match  slot        time                                text\n",
       "0      0     9  1808.40822                                100%\n",
       "1      1     0  -131.14018              twitch.tv/rage_channel\n",
       "2      1     0  -121.60481  https://www.twitch.tv/rage_channel\n",
       "3      1     0   700.72893  https://www.twitch.tv/rage_channel\n",
       "4      1     0   702.99503  https://www.twitch.tv/rage_channel"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_eng = pd.read_csv('engDfWithSenti.csv')[['match', 'slot', 'time', 'text']]\n",
    "large_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6921688, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6914913, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing links\n",
    "large_eng = large_eng[~large_eng['text'].str.contains(\"(\\.tv)\")][~large_eng['text'].str.contains(\"(\\.com)\")]\n",
    "large_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79163834, 93020265)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Forming and training the Word2Vec model\n",
    "texts = large_eng['text'].str.split(' ').values\n",
    "model = Word2Vec(sg=1, min_count=1, window=3, size=50, workers=4)\n",
    "model.build_vocab(texts)\n",
    "model.train(sentences=texts, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trahs', 0.9620177745819092),\n",
       " ('trsah', 0.9531400203704834),\n",
       " ('rubbish', 0.9508822560310364),\n",
       " ('thrash', 0.9503445625305176),\n",
       " ('garbage', 0.9502580165863037),\n",
       " ('trsh', 0.9492931365966797),\n",
       " ('retard', 0.9477963447570801),\n",
       " ('dogshit', 0.9467427730560303),\n",
       " ('tard', 0.9451007843017578),\n",
       " ('tash', 0.9420121908187866)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('trash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nub', 0.9675301313400269),\n",
       " ('idiot', 0.9619877934455872),\n",
       " ('nob', 0.9607500433921814),\n",
       " ('nooob', 0.9558690786361694),\n",
       " ('nab', 0.95162034034729),\n",
       " ('nobo', 0.9505505561828613),\n",
       " ('noooob', 0.9374358654022217),\n",
       " ('retard', 0.9371263384819031),\n",
       " ('tard', 0.9353867769241333),\n",
       " ('trahs', 0.9339444041252136)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('noob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faggot', 0.9715926647186279),\n",
       " ('cunt', 0.9637824296951294),\n",
       " ('fag', 0.9553071856498718),\n",
       " ('cuck', 0.9494332671165466),\n",
       " ('slut', 0.9432471394538879),\n",
       " ('asshole', 0.9413410425186157),\n",
       " ('twat', 0.9412334561347961),\n",
       " ('dickhead', 0.9386234283447266),\n",
       " ('pig', 0.9354640245437622),\n",
       " ('fagget', 0.9351155757904053)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('nigger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9240302"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('nigger', 'chink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fag', 0.9751840829849243),\n",
       " ('cunt', 0.972036600112915),\n",
       " ('nigger', 0.9715927243232727),\n",
       " ('asshole', 0.9545539617538452),\n",
       " ('cuck', 0.9481619596481323),\n",
       " ('dickhead', 0.945597231388092),\n",
       " ('dumbass', 0.945541501045227),\n",
       " ('bastard', 0.9450997710227966),\n",
       " ('bitch', 0.9446184039115906),\n",
       " ('shitter', 0.9419007897377014)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('faggot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Although it shows that racial slurs are similar (one even more similar than the actual equivalent of 'faggot'), it still shows that this term is similar to other words within the same disciminatory category, and it also shows only offensive language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576437"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now that there is a model applied to the Dota dataset, it can be compared to a more generalizable corpus. The GloVe Common Crawl word vectors will be used.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the GloVe Common Crawl (840B) Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe Pre-trained word vectors:\n",
    "*Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "## convert GloVe vectors in text format into the word2vec text format\n",
    "# glove2word2vec(glove_input_file='glove.840B.300d.txt', word2vec_output_file=\"gensim_glove_vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn word2vec txt into model\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('noobs', 0.8388797044754028),\n",
       " ('n00b', 0.8300445675849915),\n",
       " ('newb', 0.7779605984687805),\n",
       " ('noobie', 0.6746147274971008),\n",
       " ('Noob', 0.6667271852493286),\n",
       " ('dumbass', 0.6534879803657532),\n",
       " ('rofl', 0.6520781517028809),\n",
       " ('n00bs', 0.6475838422775269),\n",
       " ('noobish', 0.6475257873535156),\n",
       " ('stfu', 0.6416695713996887)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar('noob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6285359"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.similarity('noob', 'newbie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('newbies', 0.791803240776062),\n",
       " ('newb', 0.7092509269714355),\n",
       " ('novice', 0.6995048522949219),\n",
       " ('Newbie', 0.6929636597633362),\n",
       " ('beginner', 0.6885488033294678),\n",
       " ('newby', 0.6439059972763062),\n",
       " ('noob', 0.6285358667373657),\n",
       " ('noobie', 0.6156291961669922),\n",
       " ('newbee', 0.6093085408210754),\n",
       " ('n00b', 0.5996484756469727)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar('newbie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.511657"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.similarity('nigger', 'chink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Google News model marked their similarity as .07. The Word2Vec model trained on the Dota dataset marked their similarity as .92.\n",
    "* Given that this corpus has around 1.5 million more unique words than the Dota dataset, a large discrapency in similarity is bound to happen. With this in consideration, the GloVe Common Crawl model does pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fag', 0.9149971008300781),\n",
       " ('faggots', 0.834093451499939),\n",
       " ('dumbass', 0.8189541101455688),\n",
       " ('fagget', 0.8078358173370361),\n",
       " ('stfu', 0.7915732860565186),\n",
       " ('dipshit', 0.7823353409767151),\n",
       " ('fags', 0.7777252197265625),\n",
       " ('douchebag', 0.7625726461410522),\n",
       " ('fucker', 0.7555729150772095),\n",
       " ('nigger', 0.7529041767120361)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar('faggot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similar to the most_similar words from the Word2Vec model trained on the Dota dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196016"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Overall, the GloVe Common Crawl word vectors are more than sufficient for this project and will continue to be used.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Word Embeddings into the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Jigsaw Classifier - Adding word embeddings with simple averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"jigsaw_train.csv\")\n",
    "comments['comment_text'] = comments['comment_text'].str.replace(\"\\n\", \" \")\n",
    "test = pd.read_csv('jigsaw_test.csv')\n",
    "\n",
    "def num_upper(text):\n",
    "    \"\"\"Returns the number of capital letters in a string.\"\"\"\n",
    "    num = 0\n",
    "    for i in text:\n",
    "        if i.isupper():\n",
    "            num += 1\n",
    "    return num\n",
    "\n",
    "def vector_mean(text):\n",
    "    \"\"\"Gets the vector mean of a sentence by averaging the word vectors (each singular dimension).\"\"\"\n",
    "    sentence = []\n",
    "    words = text.split(\" \")\n",
    "    words = [word for word in words if word in glove_model.wv.vocab]\n",
    "    for word in words:\n",
    "        sentence.append(glove_model[word])\n",
    "    if len(sentence) > 0:\n",
    "        return sum(sentence)/len(sentence)\n",
    "    else:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>len</th>\n",
       "      <th>proportion of caps</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042129</td>\n",
       "      <td>0.079812</td>\n",
       "      <td>-0.001525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180881</td>\n",
       "      <td>-0.016054</td>\n",
       "      <td>0.094348</td>\n",
       "      <td>-0.038313</td>\n",
       "      <td>0.045927</td>\n",
       "      <td>-0.096478</td>\n",
       "      <td>-0.100205</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.129015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daww He matches this background colour Im seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>-0.026112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055595</td>\n",
       "      <td>0.125816</td>\n",
       "      <td>-0.024587</td>\n",
       "      <td>-0.023318</td>\n",
       "      <td>0.007922</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>-0.124093</td>\n",
       "      <td>0.073629</td>\n",
       "      <td>-0.065272</td>\n",
       "      <td>0.164368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man Im really not trying to edit war Its j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036686</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>-0.098101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217445</td>\n",
       "      <td>0.031593</td>\n",
       "      <td>-0.056462</td>\n",
       "      <td>-0.026338</td>\n",
       "      <td>0.053650</td>\n",
       "      <td>-0.013848</td>\n",
       "      <td>-0.097436</td>\n",
       "      <td>-0.015379</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.164947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation Why the edits made under my userna...      0             0   \n",
       "1  Daww He matches this background colour Im seem...      0             0   \n",
       "2  Hey man Im really not trying to edit war Its j...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate       len  proportion of caps  \\\n",
       "0        0       0       0              0  0.042129            0.079812   \n",
       "1        0       0       0              0  0.015924            0.096386   \n",
       "2        0       0       0              0  0.036686            0.021505   \n",
       "\n",
       "          0  ...       290       291       292       293       294       295  \\\n",
       "0 -0.001525  ... -0.180881 -0.016054  0.094348 -0.038313  0.045927 -0.096478   \n",
       "1 -0.026112  ... -0.055595  0.125816 -0.024587 -0.023318  0.007922  0.032877   \n",
       "2 -0.098101  ... -0.217445  0.031593 -0.056462 -0.026338  0.053650 -0.013848   \n",
       "\n",
       "        296       297       298       299  \n",
       "0 -0.100205  0.002410  0.001732  0.129015  \n",
       "1 -0.124093  0.073629 -0.065272  0.164368  \n",
       "2 -0.097436 -0.015379  0.001089  0.164947  \n",
       "\n",
       "[3 rows x 309 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning and adding features\n",
    "comments_copy = comments.copy()\n",
    "comments_copy['comment_text'] = comments_copy['comment_text'].str.replace(r\"[(\\.),(\\|)!:='&(\\*)(\\\")]\", \"\")\n",
    "comments_copy['comment_text'] = comments_copy['comment_text'].str.replace(\"\\n\", \"\")\n",
    "comments_copy['len'] = comments_copy['comment_text'].apply(len) - comments_copy['comment_text'].str.count(\" \")\n",
    "comments_copy['caps'] = comments_copy['comment_text'].apply(num_upper)\n",
    "comments_copy['proportion of caps'] = comments_copy['caps'] / comments_copy['len']\n",
    "len_min = comments_copy['len'].min()\n",
    "len_max = comments_copy['len'].max()\n",
    "comments_copy['len'] = (comments_copy['len'].values - len_min) / (len_max - len_min)\n",
    "comments_copy['proportion of caps'] = comments_copy['proportion of caps'].fillna(0)\n",
    "comments_copy = comments_copy.drop(['id', 'caps'], axis=1)\n",
    "\n",
    "# New - adding the 300-dimension vector means to the df\n",
    "comments_copy['vector mean'] = comments_copy['comment_text'].apply(vector_mean)\n",
    "tmp = pd.DataFrame(comments_copy['vector mean'].tolist())\n",
    "comments_copy = comments_copy.join(tmp)\n",
    "comments_copy = comments_copy.drop('vector mean', axis=1)\n",
    "comments_copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and adding features\n",
    "testing = test.copy()\n",
    "testing['comment_text'] = testing['comment_text'].str.replace(r\"[(\\.),(\\|)!:='&(\\*)(\\\")]\", \"\")\n",
    "testing['comment_text'] = testing['comment_text'].str.replace(\"\\n\", \"\")\n",
    "testing['len'] = testing['comment_text'].apply(len) - testing['comment_text'].str.count(\" \")\n",
    "testing['caps'] = testing['comment_text'].apply(num_upper)\n",
    "testing['proportion of caps'] = testing['caps'] / testing['len']\n",
    "len_min = testing['len'].min()\n",
    "len_max = testing['len'].max()\n",
    "testing['len'] = (testing['len'].values - len_min) / (len_max - len_min)\n",
    "testing['proportion of caps'] = testing['proportion of caps'].fillna(0)\n",
    "testing = testing.drop(['id', 'caps'], axis=1)\n",
    "\n",
    "# New - adding the 300-dimension vector means to the df\n",
    "testing['vector mean'] = testing['comment_text'].apply(vector_mean)\n",
    "tmp = pd.DataFrame(testing['vector mean'].tolist())\n",
    "testing = testing.join(tmp)\n",
    "testing = testing.drop('vector mean', axis=1)\n",
    "\n",
    "# Tfidf\n",
    "train_text = comments['comment_text']\n",
    "test_text = test['comment_text']\n",
    "text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='word', \n",
    "                                  token_pattern=r'\\w{1,}', ngram_range=(1, 1), max_features=30000)\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', \n",
    "                                  ngram_range=(1, 4), max_features=30000)\n",
    "\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=2)\n",
    "vectorizer.fit(text)\n",
    "\n",
    "train_vector = vectorizer.transform(train_text)\n",
    "test_vector = vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all features\n",
    "final_training = hstack([train_vector, comments_copy.iloc[:,7:]])\n",
    "final_testing = hstack([test_vector, testing.iloc[:,1:]])\n",
    "\n",
    "# Logistic Regression\n",
    "labels = comments.iloc[:,2:]\n",
    "results = {}\n",
    "for i in range(len(labels.columns)):\n",
    "    lr = LogisticRegression(random_state=42, solver='sag').fit(final_training, labels.iloc[:,i])\n",
    "    results[labels.columns[i]] = lr.predict_proba(final_testing)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.999823</td>\n",
       "      <td>0.233773</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>0.069227</td>\n",
       "      <td>0.977106</td>\n",
       "      <td>0.318873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>0.247398</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.031778</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.008562</td>\n",
       "      <td>0.000884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>0.040743</td>\n",
       "      <td>0.004443</td>\n",
       "      <td>0.014421</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.012678</td>\n",
       "      <td>0.009167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.000347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>0.010218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>0.958987</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.534274</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.273087</td>\n",
       "      <td>0.003169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id     toxic  severe_toxic   obscene    threat  \\\n",
       "0       00001cee341fdb12  0.999823      0.233773  0.999384  0.069227   \n",
       "1       0000247867823ef7  0.003770      0.000190  0.001786  0.000028   \n",
       "2       00013b17ad220c46  0.003311      0.003861  0.002719  0.003828   \n",
       "3       00017563c3f7919a  0.004570      0.000978  0.002590  0.000364   \n",
       "4       00017695ad8997eb  0.010297      0.001680  0.001770  0.000240   \n",
       "...                  ...       ...           ...       ...       ...   \n",
       "153159  fffcd0960ee309b5  0.247398      0.000401  0.031778  0.000061   \n",
       "153160  fffd7a9a6eb32c16  0.040743      0.004443  0.014421  0.004242   \n",
       "153161  fffda9e8d6fafa9e  0.002649      0.000250  0.005895  0.000087   \n",
       "153162  fffe8f1340a79fc2  0.008519      0.000772  0.004358  0.000880   \n",
       "153163  ffffce3fb183ee80  0.958987      0.000827  0.534274  0.002160   \n",
       "\n",
       "          insult  identity_hate  \n",
       "0       0.977106       0.318873  \n",
       "1       0.002995       0.000651  \n",
       "2       0.001953       0.001420  \n",
       "3       0.002927       0.000386  \n",
       "4       0.001963       0.000189  \n",
       "...          ...            ...  \n",
       "153159  0.008562       0.000884  \n",
       "153160  0.012678       0.009167  \n",
       "153161  0.001127       0.000347  \n",
       "153162  0.005213       0.010218  \n",
       "153163  0.273087       0.003169  \n",
       "\n",
       "[153164 rows x 7 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': test['id']})\n",
    "submission['toxic'] = results['toxic']\n",
    "submission['severe_toxic'] = results['severe_toxic']\n",
    "submission['obscene'] = results['obscene']\n",
    "submission['threat'] = results['threat']\n",
    "submission['insult'] = results['insult']\n",
    "submission['identity_hate'] = results['identity_hate']\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission.to_csv('submission_word_vector.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score decreased from 0.980 to 0.975. Given that this would generalize better to our data, the very minimal decrease in score is worth it. In addition, the score would not decrease as much with proper weighting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Jigsaw Classifier Applied to Dota Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>slot</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1808.40822</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-131.14018</td>\n",
       "      <td>twitch.tv/rage_channel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-121.60481</td>\n",
       "      <td>https://www.twitch.tv/rage_channel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match  slot        time                                text\n",
       "0      0     9  1808.40822                                100%\n",
       "1      1     0  -131.14018              twitch.tv/rage_channel\n",
       "2      1     0  -121.60481  https://www.twitch.tv/rage_channel"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng = pd.read_csv('engDfWithSenti.csv')[['match', 'slot', 'time', 'text']].head(20000)\n",
    "eng.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original model features\n",
    "dota_text = eng.copy()\n",
    "dota_text = dota_text[~dota_text['text'].str.contains(\"(\\.tv)\")][~dota_text['text'].str.contains(\"(\\.com)\")]\n",
    "dota_text['text'] = dota_text['text'].str.replace(r\"[(\\.),(\\|)!:='&(\\*)(\\\")]\", \"\")\n",
    "dota_text['text'] = dota_text['text'].str.replace(\"\\n\", \"\")\n",
    "dota_text['len'] = dota_text['text'].apply(len) - dota_text['text'].str.count(\" \")\n",
    "dota_text['caps'] = dota_text['text'].apply(num_upper)\n",
    "dota_text['proportion of caps'] = dota_text['caps'] / dota_text['len']\n",
    "len_min = dota_text['len'].min()\n",
    "len_max = dota_text['len'].max()\n",
    "dota_text['len'] = (dota_text['len'].values - len_min) / (len_max - len_min)\n",
    "dota_text['proportion of caps'] = dota_text['proportion of caps'].fillna(0)\n",
    "dota_text = dota_text.drop('caps', axis=1)\n",
    "\n",
    "# New - adding the 300-dimension vector means to the df\n",
    "dota_text['vector mean'] = dota_text['text'].apply(vector_mean)\n",
    "tmp = pd.DataFrame(dota_text['vector mean'].tolist())\n",
    "dota_text = dota_text.join(tmp)\n",
    "dota_text = dota_text.drop('vector mean', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>slot</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "      <th>proportion of caps</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>3017</td>\n",
       "      <td>1</td>\n",
       "      <td>1436.84913</td>\n",
       "      <td>its funny though</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>3017</td>\n",
       "      <td>1</td>\n",
       "      <td>1444.78053</td>\n",
       "      <td>mid and safelane carry on enemy team are lvl 19</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>3017</td>\n",
       "      <td>1</td>\n",
       "      <td>1450.51253</td>\n",
       "      <td>on my team not even 15</td>\n",
       "      <td>0.157407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>3017</td>\n",
       "      <td>1</td>\n",
       "      <td>1458.64383</td>\n",
       "      <td>not both anyway</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>3017</td>\n",
       "      <td>5</td>\n",
       "      <td>1466.10863</td>\n",
       "      <td>morph has lag</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       match  slot        time  \\\n",
       "19975   3017     1  1436.84913   \n",
       "19976   3017     1  1444.78053   \n",
       "19977   3017     1  1450.51253   \n",
       "19978   3017     1  1458.64383   \n",
       "19979   3017     5  1466.10863   \n",
       "\n",
       "                                                  text       len  \\\n",
       "19975                                 its funny though  0.129630   \n",
       "19976  mid and safelane carry on enemy team are lvl 19  0.351852   \n",
       "19977                           on my team not even 15  0.157407   \n",
       "19978                                  not both anyway  0.120370   \n",
       "19979                                    morph has lag  0.101852   \n",
       "\n",
       "       proportion of caps   0   1   2   3  ...  290  291  292  293  294  295  \\\n",
       "19975                 0.0 NaN NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "19976                 0.0 NaN NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "19977                 0.0 NaN NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "19978                 0.0 NaN NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "19979                 0.0 NaN NaN NaN NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "       296  297  298  299  \n",
       "19975  NaN  NaN  NaN  NaN  \n",
       "19976  NaN  NaN  NaN  NaN  \n",
       "19977  NaN  NaN  NaN  NaN  \n",
       "19978  NaN  NaN  NaN  NaN  \n",
       "19979  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 306 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dota_text[dota_text.iloc[:,6].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, only match 3017 received NAs for their sentence vectors. This may just be an anomoly. If it resurfaces, attention will be brought back to this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dota_text = dota_text[dota_text['match'] != 3017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf\n",
    "train_text = comments['comment_text']\n",
    "test_text = dota_text['text']\n",
    "text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='word', \n",
    "                                  token_pattern=r'\\w{1,}', ngram_range=(1, 1), max_features=30000)\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', \n",
    "                                  ngram_range=(1, 4), max_features=30000)\n",
    "\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=2)\n",
    "vectorizer.fit(text)\n",
    "\n",
    "train_vector = vectorizer.transform(train_text)\n",
    "test_vector = vectorizer.transform(test_text)\n",
    "\n",
    "# Combining all features\n",
    "final_training = hstack([train_vector, comments_copy.iloc[:,7:]])\n",
    "final_testing = hstack([test_vector, dota_text.iloc[:,4:]])\n",
    "\n",
    "# Logistic Regression\n",
    "labels = comments.iloc[:,2:]\n",
    "results = {}\n",
    "for i in range(len(labels.columns)):\n",
    "    lr = LogisticRegression(random_state=42, solver='sag').fit(final_training, labels.iloc[:,i])\n",
    "    results[labels.columns[i]] = lr.predict_proba(final_testing)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100%</td>\n",
       "      <td>0.034039</td>\n",
       "      <td>0.063953</td>\n",
       "      <td>0.071940</td>\n",
       "      <td>0.061007</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.035569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>carry</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.000713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yes dog</td>\n",
       "      <td>0.140038</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.044461</td>\n",
       "      <td>0.971723</td>\n",
       "      <td>0.438611</td>\n",
       "      <td>0.608974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yeah</td>\n",
       "      <td>0.076845</td>\n",
       "      <td>0.060020</td>\n",
       "      <td>0.014420</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.064132</td>\n",
       "      <td>0.038330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fast and furious</td>\n",
       "      <td>0.950317</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.483092</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.021371</td>\n",
       "      <td>0.007016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19951</th>\n",
       "      <td>report shadow</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.026290</td>\n",
       "      <td>0.004992</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19952</th>\n",
       "      <td>ironic</td>\n",
       "      <td>0.010940</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>0.001351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19953</th>\n",
       "      <td>rage boss</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.000747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19954</th>\n",
       "      <td>mame privet</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.002390</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>0.004485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19955</th>\n",
       "      <td>gg wp</td>\n",
       "      <td>0.096980</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.006607</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.001682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19931 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0                  100%  0.034039      0.063953  0.071940  0.061007  0.055936   \n",
       "7                 carry  0.026550      0.000256  0.002668  0.000012  0.001987   \n",
       "8               yes dog  0.140038      0.001870  0.044461  0.971723  0.438611   \n",
       "9                 yeah   0.076845      0.060020  0.014420  0.002525  0.064132   \n",
       "10     fast and furious  0.950317      0.001478  0.483092  0.000049  0.021371   \n",
       "...                 ...       ...           ...       ...       ...       ...   \n",
       "19951     report shadow  0.008418      0.009685  0.001736  0.026290  0.004992   \n",
       "19952            ironic  0.010940      0.001451  0.001924  0.000813  0.004775   \n",
       "19953         rage boss  0.007376      0.000585  0.001321  0.000036  0.001129   \n",
       "19954       mame privet  0.015945      0.004472  0.003854  0.002390  0.024617   \n",
       "19955             gg wp  0.096980      0.000726  0.006607  0.000579  0.002060   \n",
       "\n",
       "       identity_hate  \n",
       "0           0.035569  \n",
       "7           0.000713  \n",
       "8           0.608974  \n",
       "9           0.038330  \n",
       "10          0.007016  \n",
       "...              ...  \n",
       "19951       0.008200  \n",
       "19952       0.001351  \n",
       "19953       0.000747  \n",
       "19954       0.004485  \n",
       "19955       0.001682  \n",
       "\n",
       "[19931 rows x 7 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dota = pd.DataFrame({'text': dota_text['text']})\n",
    "labeled_dota['toxic'] = results['toxic']\n",
    "labeled_dota['severe_toxic'] = results['severe_toxic']\n",
    "labeled_dota['obscene'] = results['obscene']\n",
    "labeled_dota['threat'] = results['threat']\n",
    "labeled_dota['insult'] = results['insult']\n",
    "labeled_dota['identity_hate'] = results['identity_hate']\n",
    "labeled_dota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Jigsaw Classifier -  Adding word embeddings with weighted averaging\n",
    "* Weighting each word vector by the Tfidf score of each word in that sentence as part of a weighted average\n",
    "    * Tfidf score: measure of word importance\n",
    "        * ie. For the sentence \"the cat ran,\" \"the\" should not have the same weight as \"cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn txt of word vectors into a Word2Vec model\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"jigsaw_train.csv\")\n",
    "comments['comment_text'] = comments['comment_text'].str.replace(\"\\n\", \" \")\n",
    "test = pd.read_csv('jigsaw_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Tfidf model\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(comments['comment_text'].values)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "def get_word_weight(text):\n",
    "    \"\"\"Returns a dictionary where keys are the words of the text and values are their weights.\"\"\"\n",
    "    tfidf_matrix = vectorizer.transform([text]).todense()\n",
    "    feature_index = tfidf_matrix[0,:].nonzero()[1]\n",
    "    tfidf_scores = zip([feature_names[i] for i in feature_index], [tfidf_matrix[0, x] for x in feature_index])\n",
    "    return dict(tfidf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0.3922306315610665,\n",
       " 'girl': 0.7617353366583565,\n",
       " 'hello': 0.5156688943025236}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, I am a girl\"\n",
    "get_word_weight(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weights don't sum to 1 (they're supposed to not)\n",
    "* Since weights don't sum to 1, divide by the sum of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0.2349200057841454,\n",
       " 'girl': 0.4562286963197283,\n",
       " 'hello': 0.3088512978961263}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, I am a girl\"\n",
    "text_dict = get_word_weight(text)\n",
    "total = sum(text_dict.values())\n",
    "\n",
    "# dividing by sum of weights to have weights sum to 1\n",
    "text_dict = {key:(val/total) for key,val in text_dict.items()}\n",
    "text_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After dividing by the sum of weights, they now sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_upper(text):\n",
    "    \"\"\"Returns the number of capital letters in a string.\"\"\"\n",
    "    num = 0\n",
    "    for i in text:\n",
    "        if i.isupper():\n",
    "            num += 1\n",
    "    return num\n",
    "\n",
    "def vector_mean(text):\n",
    "    \"\"\"Gets the vector mean of a sentence by averaging the word vectors (each singular dimension).\"\"\"\n",
    "    sentence = []\n",
    "    words = text.split(\" \")\n",
    "    words = [word for word in words if word in glove_model.wv.vocab]\n",
    "    for word in words:\n",
    "        sentence.append(glove_model[word])\n",
    "    if len(sentence) > 0:\n",
    "        return sum(sentence)/len(sentence)\n",
    "    else:\n",
    "        return np.zeros(300)\n",
    "    \n",
    "def weighted_vector_mean(text):\n",
    "    \"\"\"Gets the weighted vector mean of a sentence by averaging the word vectors according to Tfidf weights.\"\"\"\n",
    "    sentence_vects = []\n",
    "    sentence_weights = []\n",
    "    words = text.split(\" \")\n",
    "    words = [word for word in words if word in glove_model.wv.vocab]\n",
    "    \n",
    "    text_dict = get_word_weight(text)\n",
    "    total = sum(text_dict.values())\n",
    "    text_dict = {key:(val/total) for key,val in text_dict.items()}\n",
    "        \n",
    "    for word in words:\n",
    "        sentence_vects.append(glove_model[word])               # get word vectors\n",
    "        if word.lower() in text_dict.keys():\n",
    "            sentence_weights.append(text_dict[word.lower()])   # get weights of words\n",
    "        else:\n",
    "            sentence_weights.append(0)\n",
    "        \n",
    "    if len(sentence_vects) > 0:\n",
    "        return np.transpose(sentence_vects) @ sentence_weights / len(sentence_vects)\n",
    "    else:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>len</th>\n",
       "      <th>proportion of caps</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042129</td>\n",
       "      <td>0.079812</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003081</td>\n",
       "      <td>-0.001262</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>-0.001422</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-0.002976</td>\n",
       "      <td>-0.003383</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>-0.000352</td>\n",
       "      <td>0.002616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daww He matches this background colour Im seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>-0.004279</td>\n",
       "      <td>-0.004179</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>-0.006293</td>\n",
       "      <td>-0.001872</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.006684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man Im really not trying to edit war Its j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036686</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005206</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>-0.002285</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>-0.000395</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.003905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation Why the edits made under my userna...      0             0   \n",
       "1  Daww He matches this background colour Im seem...      0             0   \n",
       "2  Hey man Im really not trying to edit war Its j...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate       len  proportion of caps  \\\n",
       "0        0       0       0              0  0.042129            0.079812   \n",
       "1        0       0       0              0  0.015924            0.096386   \n",
       "2        0       0       0              0  0.036686            0.021505   \n",
       "\n",
       "          0  ...       290       291       292       293       294       295  \\\n",
       "0  0.000552  ... -0.003081 -0.001262  0.002846 -0.001422  0.000968 -0.002976   \n",
       "1  0.002804  ... -0.009804  0.006116 -0.004279 -0.004179  0.000589  0.004729   \n",
       "2 -0.002817  ... -0.005206  0.000549 -0.002285 -0.000499  0.000947 -0.000120   \n",
       "\n",
       "        296       297       298       299  \n",
       "0 -0.003383  0.000153 -0.000352  0.002616  \n",
       "1 -0.006293 -0.001872  0.002173  0.006684  \n",
       "2 -0.002729 -0.000395 -0.000113  0.003905  \n",
       "\n",
       "[3 rows x 309 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning and adding features\n",
    "comments_copy = comments.copy()\n",
    "comments_copy['comment_text'] = comments_copy['comment_text'].str.replace(r\"[(\\.),(\\|)!:='&(\\*)(\\\")]\", \"\")\n",
    "comments_copy['comment_text'] = comments_copy['comment_text'].str.replace(\"\\n\", \"\")\n",
    "comments_copy['len'] = comments_copy['comment_text'].apply(len) - comments_copy['comment_text'].str.count(\" \")\n",
    "comments_copy['caps'] = comments_copy['comment_text'].apply(num_upper)\n",
    "comments_copy['proportion of caps'] = comments_copy['caps'] / comments_copy['len']\n",
    "len_min = comments_copy['len'].min()\n",
    "len_max = comments_copy['len'].max()\n",
    "comments_copy['len'] = (comments_copy['len'].values - len_min) / (len_max - len_min)\n",
    "comments_copy['proportion of caps'] = comments_copy['proportion of caps'].fillna(0)\n",
    "comments_copy = comments_copy.drop(['id', 'caps'], axis=1)\n",
    "\n",
    "# New - adding the 300D vector means, weighted by Tfidf weights\n",
    "comments_copy['vector mean'] = comments_copy['comment_text'].apply(weighted_vector_mean)\n",
    "tmp = pd.DataFrame(comments_copy['vector mean'].tolist())\n",
    "comments_copy = comments_copy.join(tmp)\n",
    "comments_copy = comments_copy.drop('vector mean', axis=1)\n",
    "comments_copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and adding features\n",
    "testing = test.copy()\n",
    "testing['comment_text'] = testing['comment_text'].str.replace(r\"[(\\.),(\\|)!:='&(\\*)(\\\")]\", \"\")\n",
    "testing['comment_text'] = testing['comment_text'].str.replace(\"\\n\", \"\")\n",
    "testing['len'] = testing['comment_text'].apply(len) - testing['comment_text'].str.count(\" \")\n",
    "testing['caps'] = testing['comment_text'].apply(num_upper)\n",
    "testing['proportion of caps'] = testing['caps'] / testing['len']\n",
    "len_min = testing['len'].min()\n",
    "len_max = testing['len'].max()\n",
    "testing['len'] = (testing['len'].values - len_min) / (len_max - len_min)\n",
    "testing['proportion of caps'] = testing['proportion of caps'].fillna(0)\n",
    "testing = testing.drop(['id', 'caps'], axis=1)\n",
    "\n",
    "# New - adding the 300D vector means, weighted by Tfidf weights\n",
    "testing['vector mean'] = testing['comment_text'].apply(weighted_vector_mean)\n",
    "tmp = pd.DataFrame(testing['vector mean'].tolist())\n",
    "testing = testing.join(tmp)\n",
    "testing = testing.drop('vector mean', axis=1)\n",
    "\n",
    "# Tfidf\n",
    "train_text = comments['comment_text']\n",
    "test_text = test['comment_text']\n",
    "text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='word', \n",
    "                                  token_pattern=r'\\w{1,}', ngram_range=(1, 1), max_features=30000)\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', \n",
    "                                  ngram_range=(1, 4), max_features=30000)\n",
    "\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=2)\n",
    "vectorizer.fit(text)\n",
    "\n",
    "train_vector = vectorizer.transform(train_text)\n",
    "test_vector = vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all features\n",
    "final_training = hstack([train_vector, comments_copy.iloc[:,7:]])\n",
    "final_testing = hstack([test_vector, testing.iloc[:,1:]])\n",
    "\n",
    "# Logistic Regression - applying the model on the dota data\n",
    "labels = comments.iloc[:,2:]\n",
    "results = {}\n",
    "for i in range(len(labels.columns)):\n",
    "    lr = LogisticRegression(random_state=42, solver='sag').fit(final_training, labels.iloc[:,i])\n",
    "    results[labels.columns[i]] = lr.predict_proba(final_testing)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.999786</td>\n",
       "      <td>0.207811</td>\n",
       "      <td>0.999486</td>\n",
       "      <td>0.046438</td>\n",
       "      <td>0.982040</td>\n",
       "      <td>0.280475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>0.001652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.010267</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.006372</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.002105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.000550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.011063</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.999786      0.207811  0.999486  0.046438  0.982040   \n",
       "1  0000247867823ef7  0.003983      0.001345  0.002120  0.000317  0.003159   \n",
       "2  00013b17ad220c46  0.010267      0.004232  0.006372  0.002058  0.003781   \n",
       "3  00017563c3f7919a  0.003149      0.001381  0.002399  0.000784  0.003045   \n",
       "4  00017695ad8997eb  0.011063      0.001115  0.002186  0.000589  0.002885   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.280475  \n",
       "1       0.001652  \n",
       "2       0.002105  \n",
       "3       0.000550  \n",
       "4       0.000488  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': test['id']})\n",
    "submission['toxic'] = results['toxic']\n",
    "submission['severe_toxic'] = results['severe_toxic']\n",
    "submission['obscene'] = results['obscene']\n",
    "submission['threat'] = results['threat']\n",
    "submission['insult'] = results['insult']\n",
    "submission['identity_hate'] = results['identity_hate']\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission.to_csv('submission_word_vector.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This submission scored .977. The model with the simple averaged vectors scored .975. The model without any word embeddings scored .980. As mentioned before, the very minimal decrease in score is a worth tradeoff for better generalization to our data and to unseen data. Instead of a .005 score decrease, now there is only a .003 score decrease.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL Updated Jigsaw Classifier Applied to Dota Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>slot</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1808.40822</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-131.14018</td>\n",
       "      <td>twitch.tv/rage_channel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-121.60481</td>\n",
       "      <td>https://www.twitch.tv/rage_channel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match  slot        time                                text\n",
       "0      0     9  1808.40822                                100%\n",
       "1      1     0  -131.14018              twitch.tv/rage_channel\n",
       "2      1     0  -121.60481  https://www.twitch.tv/rage_channel"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng = pd.read_csv('engDfWithSenti.csv')[['match', 'slot', 'time', 'text']].head(20000)\n",
    "eng.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original model features\n",
    "dota_text = eng.copy()\n",
    "dota_text = dota_text[~dota_text['text'].str.contains(\"(\\.tv)\")][~dota_text['text'].str.contains(\"(\\.com)\")]\n",
    "dota_text['text'] = dota_text['text'].str.replace(r\"[(\\.),(\\|)!:='&(\\*)(\\\")]\", \"\")\n",
    "dota_text['text'] = dota_text['text'].str.replace(\"\\n\", \"\")\n",
    "dota_text['len'] = dota_text['text'].apply(len) - dota_text['text'].str.count(\" \")\n",
    "dota_text['caps'] = dota_text['text'].apply(num_upper)\n",
    "dota_text['proportion of caps'] = dota_text['caps'] / dota_text['len']\n",
    "len_min = dota_text['len'].min()\n",
    "len_max = dota_text['len'].max()\n",
    "dota_text['len'] = (dota_text['len'].values - len_min) / (len_max - len_min)\n",
    "dota_text['proportion of caps'] = dota_text['proportion of caps'].fillna(0)\n",
    "dota_text = dota_text.drop('caps', axis=1)\n",
    "\n",
    "# New - loading Tfidf model\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(dota_text['text'].values)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# New - adding the 300-dimension vector means to the df\n",
    "dota_text['vector mean'] = dota_text['text'].apply(weighted_vector_mean)\n",
    "tmp = pd.DataFrame(dota_text['vector mean'].tolist())\n",
    "dota_text = dota_text.join(tmp).dropna()\n",
    "dota_text = dota_text.drop('vector mean', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf\n",
    "train_text = comments['comment_text']\n",
    "test_text = dota_text['text']\n",
    "text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='word', \n",
    "                                  token_pattern=r'\\w{1,}', ngram_range=(1, 1), max_features=30000)\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', \n",
    "                                  ngram_range=(1, 4), max_features=30000)\n",
    "\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=2)\n",
    "vectorizer.fit(text)\n",
    "\n",
    "train_vector = vectorizer.transform(train_text)\n",
    "test_vector = vectorizer.transform(test_text)\n",
    "\n",
    "# Combining all features\n",
    "final_training = hstack([train_vector, comments_copy.iloc[:,7:]])\n",
    "final_testing = hstack([test_vector, dota_text.iloc[:,4:]])\n",
    "\n",
    "# Logistic Regression\n",
    "labels = comments.iloc[:,2:]\n",
    "results = {}\n",
    "for i in range(len(labels.columns)):\n",
    "    lr = LogisticRegression(random_state=42, solver='sag').fit(final_training, labels.iloc[:,i])\n",
    "    results[labels.columns[i]] = lr.predict_proba(final_testing)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dota = pd.DataFrame({'text': dota_text['text']})\n",
    "labeled_dota['toxic'] = results['toxic']\n",
    "labeled_dota['severe_toxic'] = results['severe_toxic']\n",
    "labeled_dota['obscene'] = results['obscene']\n",
    "labeled_dota['threat'] = results['threat']\n",
    "labeled_dota['insult'] = results['insult']\n",
    "labeled_dota['identity_hate'] = results['identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_dota.to_csv('tfidf_weighted_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next session:\n",
    "* Understand and handle the false positives\n",
    "* Insights on toxic players and players in general"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
